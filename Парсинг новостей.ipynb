{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import threading\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87273e7c",
   "metadata": {},
   "source": [
    "##   РИА Новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc73f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def get_ria_links():\n",
    "    base_url = \"https://ria.ru/services/search/getmore/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://ria.ru/\",\n",
    "        \"Accept-Language\": \"ru-RU,ru;q=0.9\"\n",
    "    }\n",
    "    links = []\n",
    "    \n",
    "    start_date = datetime(2025, 1, 1)\n",
    "    end_date = datetime(2025, 5, 23)\n",
    "    delta = timedelta(days=2)\n",
    "    \n",
    "    while start_date < end_date:\n",
    "        next_date = min(start_date + delta, end_date)\n",
    "        params = {\n",
    "            \"query\": \"новости\",\n",
    "            \"date_from\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"date_to\": next_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"sort\": \"date\",\n",
    "            \"offset\": 0\n",
    "        }\n",
    "        \n",
    "        while True:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            new_links = [a[\"href\"] for a in soup.find_all(\"a\", class_=\"list-item__title\")]\n",
    "            if not new_links:\n",
    "                break\n",
    "            links.extend(new_links)\n",
    "            params[\"offset\"] += len(new_links)\n",
    "            time.sleep(4)  \n",
    "        \n",
    "        start_date = next_date + timedelta(days=1)\n",
    "    \n",
    "    return links\n",
    "\n",
    "\n",
    "\n",
    "ria_links = get_ria_links()\n",
    "\n",
    "\n",
    "print(\"РИА Новости:\", len(ria_links), \"ссылок\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArticle_ria(url0):\n",
    "    \"\"\"\n",
    "    Возвращает кортеж с URL, датой публикации, заголовком, тегами, рубрикой и текстом статьи.\n",
    "    \"\"\"\n",
    "    page0 = requests.get(url0)\n",
    "    if page0.status_code != 200:\n",
    "        return url0, None, None, None, None, None \n",
    "\n",
    "    soup0 = BeautifulSoup(page0.text, 'html.parser')\n",
    "\n",
    "    title = soup0.find('meta', attrs={'name': 'analytics:title'})\n",
    "    tag = soup0.find('meta', attrs={'name': 'analytics:tags'})\n",
    "    rubric = soup0.find('meta', attrs={'name': 'analytics:rubric'})\n",
    "\n",
    "    title = title['content'] if title else None\n",
    "    tag = tag['content'] if tag else None\n",
    "    rubric = rubric['content'] if rubric else None\n",
    "\n",
    "    article_texts = soup0.find_all('div', class_='article__text')\n",
    "    final_text = \" \".join([text.get_text(strip=True) for text in article_texts]) if article_texts else None\n",
    "\n",
    "    date_published = None\n",
    "    time_tag = soup0.find(attrs={\"data-published-time\": True})\n",
    "\n",
    "    if time_tag:\n",
    "        raw_date = time_tag[\"data-published-time\"]  \n",
    "        try:\n",
    "            parsed_date = datetime.strptime(raw_date, \"%Y%m%dT%H%M\")  \n",
    "            date_published = parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        except ValueError:\n",
    "            date_published = None \n",
    "    \n",
    "    return url0, date_published, title, tag, rubric, final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ria = [] \n",
    "\n",
    "for lin in ria_links:\n",
    "    res = GetArticle_ria(lin)\n",
    "    news_ria.append(res)\n",
    "        \n",
    "    sleep(4)\n",
    "    \n",
    "df = pd.DataFrame(news_ria)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9213343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"ria1.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Файл ria.csv сохранен.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42312a7f",
   "metadata": {},
   "source": [
    "##   Лента.ру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lenta_links():\n",
    "    base_url = \"https://lenta.ru/parts/news/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"}\n",
    "    links = []\n",
    "    page = 1\n",
    "    total_pages = 10\n",
    "    while page <= total_pages:\n",
    "        response = requests.get(base_url + f\"{page}/\", headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        new_links = [\"https://lenta.ru\" + a[\"href\"] for a in soup.find_all(\"a\", href=True) if a[\"href\"].startswith(\"/news/\")]\n",
    "        if not new_links:\n",
    "            break\n",
    "        links.extend(new_links)\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "    print()\n",
    "    return links\n",
    "lenta_links = get_lenta_links()\n",
    "print(\"Лента.ру:\", len(lenta_links), \"ссылок\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArticle_lenta(url0):\n",
    "    \"\"\"\n",
    "    Возвращает кортеж с URL, датой публикации, заголовком, тегами, рубрикой и текстом статьи.\n",
    "    \"\"\"\n",
    "    page0 = requests.get(url0)\n",
    "    if page0.status_code != 200:\n",
    "        return url0, None, None, None, None, None \n",
    "\n",
    "    soup0 = BeautifulSoup(page0.text, 'html.parser')\n",
    "\n",
    "    rubric_tag = soup0.find('div', class_='rubric-header__title')\n",
    "    rubric = rubric_tag.get_text(strip=True) if rubric_tag else None\n",
    "    \n",
    "    title_i = soup0.find('span', class_='topic-body__title')\n",
    "    title = title_i.get_text(strip=True) if title_i else None\n",
    "    \n",
    "    tag = None\n",
    "    \n",
    "    authors_i = soup0.find('span', class_='topic-authors__name')\n",
    "    authors = authors_i.get_text(strip=True) if authors_i else None\n",
    "\n",
    "    article_texts = soup0.find_all('p', class_='topic-body__content-text')\n",
    "    final_text = \" \".join([text.get_text(strip=True) for text in article_texts]) if article_texts else None\n",
    "\n",
    "    date_published = None\n",
    "    time_tag = soup0.find(\"a\", class_=\"topic-header__item topic-header__time\")\n",
    "\n",
    "    if time_tag:\n",
    "        raw_date = time_tag.get_text(strip=True)  \n",
    "\n",
    "       \n",
    "        month_translate = {\n",
    "            \"января\": \"January\", \"февраля\": \"February\", \"марта\": \"March\",\n",
    "            \"апреля\": \"April\", \"мая\": \"May\", \"июня\": \"June\",\n",
    "            \"июля\": \"July\", \"августа\": \"August\", \"сентября\": \"September\",\n",
    "            \"октября\": \"October\", \"ноября\": \"November\", \"декабря\": \"December\"\n",
    "        }\n",
    "\n",
    "        \n",
    "        parts = raw_date.split()\n",
    "        if len(parts) == 4:  \n",
    "            parts[2] = month_translate.get(parts[2], parts[2]) \n",
    "            clean_date = \" \".join(parts)  \n",
    "\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(clean_date, \"%H:%M, %d %B %Y\")\n",
    "                date_published = parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Ошибка при разборе даты: {e}\")\n",
    "                date_published = None\n",
    "    \n",
    "    return url0, date_published, title, tag, rubric, final_text, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_lenta = [] \n",
    "\n",
    "for lin in lenta_links:\n",
    "    res = GetArticle_lenta(lin)\n",
    "    news_lenta.append(res)\n",
    "        \n",
    "    sleep(2)\n",
    "    \n",
    "df = pd.DataFrame(news_lenta)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e798f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lenta1.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Файл lenta.csv сохранен.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e122563",
   "metadata": {},
   "source": [
    "##   РБК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f59971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rbk_links():\n",
    "    base_url = \"https://www.rbc.ru/short_news\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(base_url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    links = set()\n",
    "    scroll_pause_time = 8\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if \"https://www.rbc.ru/\" in href:\n",
    "                links.add(href)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    driver.quit()\n",
    "    return list(links)\n",
    "\n",
    "rbk_links = get_rbk_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfccf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArticle_rbk(url0):\n",
    "    \"\"\"\n",
    "    Возвращает кортеж с url0, date, author, description, title, final_text, rubrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page0 = requests.get(url0)\n",
    "        page0.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Ошибка при запросе страницы: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup0 = BeautifulSoup(page0.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        tag = soup0.find('meta', attrs={'name': 'news_keywords'}).get('content')\n",
    "    except AttributeError:\n",
    "        tag = None\n",
    "\n",
    "    try:\n",
    "        title = soup0.find('meta', {'property': 'og:title'}).get('content')\n",
    "    except AttributeError:\n",
    "        title = None\n",
    "\n",
    "    try:\n",
    "        rubric = soup0.find('meta', {'property': 'og:site_name'}).get('content')\n",
    "    except AttributeError:\n",
    "        rubric = None\n",
    "\n",
    "    try:\n",
    "        text = soup0.find_all('p')\n",
    "        cleaned_text = [element.get_text().strip() for element in text]\n",
    "        final_text = ' '.join(cleaned_text)\n",
    "    except AttributeError:\n",
    "        final_text = None\n",
    "\n",
    "    try:\n",
    "        time_tag = soup0.find(\"time\", class_=\"article__header__date\")\n",
    "        if time_tag and \"datetime\" in time_tag.attrs:\n",
    "            iso_date = time_tag[\"datetime\"]\n",
    "            parsed_date = datetime.fromisoformat(iso_date)\n",
    "            date = parsed_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            date = None\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "\n",
    "    return url0, date, tag, title, rubric, final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_rbk = []\n",
    "\n",
    "for lin in rbk_links:\n",
    "    res = GetArticle_rbk(lin)\n",
    "    news_rbk.append(res)\n",
    "        \n",
    "    sleep(2)\n",
    "    \n",
    "df = pd.DataFrame(news_rbk)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"rbk1.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Файл rbk.csv сохранен.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
